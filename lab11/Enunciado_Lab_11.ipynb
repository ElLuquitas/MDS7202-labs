{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
    "\n",
    "- Nombre de alumno 1: Elizabeth Ramírez Z.\n",
    "- Nombre de alumno 2: Lucas Orellana J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/ElLuquitas/MDS7202-labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_actions: 2\n",
      "dim_states: MultiDiscrete([32 11  2])\n"
     ]
    }
   ],
   "source": [
    "dim_actions = env.action_space.n # dimensión de las acciones\n",
    "dim_states = env.observation_space # dimensión de los estados (observaciones)\n",
    "\n",
    "print('dim_actions:', dim_actions)\n",
    "print('dim_states:', dim_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente **Blackjack** consiste en una recreación del juego de cartas del mismo nombre, en donde el agente busca vencer al *dealer* obteniendo un conjunto de cartas que sumen $21$ (sin sobrepasar este número) a la vez que esta suma es mayor a la obtenida por el *dealer*. Su formulación en MDP consta de 3 puntos clave:\n",
    "\n",
    "- Estados: cada uno de los estados está descrito como una tupla de tres elementos `(sum_hand, dealer_card, usable_ace)`, en donde:\n",
    "    - `sum_hand` representa la suma de las cartas actuales del agente, con un total de $32$ posibilidades.\n",
    "    - `dealer_card` representa la carta boca arriba con la que empieza el *dealer*, con un total de $11$ posibilidades.\n",
    "    - `usable_ace` representa si el agente siene algún as, el cual puede contar como $11$ en caso de que la suma (considerando el as como $11$) no exceda el valor $21$, con un total de $2$ posibilidades.\n",
    "\n",
    "    Esto nos da un total de $32 \\cdot 11 \\cdot 2 = 704$ estados.\n",
    "\n",
    "- Acciones: Dada la naturaleza del juego, el agente puede ejecutar dos acciones:\n",
    "    - `0` significa que el agente se queda con las cartas que tiene en mano (stick)\n",
    "    - `1` significa que el agente toma auna carta adicional (hit)\n",
    "\n",
    "- Recompensas: Las recompensas tienen que ver directamnte con el resultado del juego:\n",
    "    - `+1` si el agente le gana al *dealer* sin un blackjack natural.\n",
    "    - `-1` si el agente pierde contra el *dealer*.\n",
    "    - `0` si ambos empatan.\n",
    "    - `+1.5` si el agente le gana al *dealer* con un blackjack natural (lo obtiene en la mano inicial)\n",
    "\n",
    "El episodio (o la experiencia) termina en dos casos:\n",
    "1. El agente decide robar una carta más y la suma de su mano excede el valor $21$.\n",
    "2. El agente se queda con su mano actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -0.41\n",
      "Desviación estándar de las recompensas: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Reiniciamos el ambiente (por si acaso)\n",
    "env.reset()\n",
    "\n",
    "# Parámetros de la simulación\n",
    "episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "# Simulación\n",
    "for _ in range(episodes):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()                      # Se toma una acción aleatoria\n",
    "        obs, reward, done, truncated, info = env.step(action)   # Se realiza la acción\n",
    "        total_reward += reward                                  # Y se acumula la recompensa del episodio\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Cálculo del promedio y la desviación estándar\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "# Reporte\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se expone en los resultados, el performance es bastante malo, ya que el agente está perdiendo, en promedio, 0.42 unidades de rrecompensa por cada episodio, lo cual indica que toma decisiones subóptimas. Además, la desviación estándar refleja la alta variabilidad en este juego: como las acciones del agente son aleatorias, se ingresa mucha incertidumbre, teniendo por resultado una alta variabilidad en los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que este es un entorno multidiscreto, lo ideal sería usar un modelo del tipo `PPO`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23395da0dd3a42abadaa9abedfa5d3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2395027f730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Iniciamos el modelo\n",
    "model_ppo = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    seed=30\n",
    ")\n",
    "\n",
    "# Reiniciamos el ambiente\n",
    "env.reset()\n",
    "\n",
    "# Parámetros de la simulación\n",
    "episodes = 5000\n",
    "model = model_ppo\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.learn(total_timesteps=50000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio tras entrenamiento: -0.03\n",
      "Desviación estándar: 0.95\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Evaluamos el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5000)\n",
    "print(f\"Recompensa promedio tras entrenamiento: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos nuevos resultados reflejan una mejora significativa en las acciones del agente. Aunque la recompensa promedio haya aumentado, sigue siendo negativa, pero por lo menos el agente ya no pierde tanto. La desviación estándar sigue siendo alta, pero es típico para estos juegos de azar.\n",
    "\n",
    "Es posible enseñarle al agente cómo proceder en algunas situaciones para disminuir esta varianza tan alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "def accion_agente(model, estado):\n",
    "    '''\n",
    "    Función que devuelve la acción que el agente tomará en un estado dado.\n",
    "\n",
    "    Args:\n",
    "        - model: Modelo entrenado.\n",
    "        - estado: Estado en el que se encuentra el agente.\n",
    "\n",
    "    Returns:\n",
    "        - acción: Acción que tomará el agente.\n",
    "    '''\n",
    "    # El estado se debe pasar como un array de una dimensión\n",
    "    estado = np.array(estado).flatten()\n",
    "\n",
    "    accion, _ = model.predict(estado, deterministic=True)\n",
    "    return accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado: [6, 7, 0]\n",
      "Acción: 1\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "estado = [6, 7, 0]\n",
    "model = model_ppo\n",
    "\n",
    "print(f\"Estado: {estado}\")\n",
    "print(f\"Acción: {accion_agente(model, estado)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este primer caso, el agente tiene 2 cartas que suman 3 puntos, mientras que el *dealer*, con una sola carta, ya tiene 7 puntos. Lo lógico en este caso es robar una carta ya que, independiente del valor que se obtenga, no sobrepasará lo 21. Es justamente esta acción que el agente decide realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado: [19, 3, 1]\n",
      "Acción: 0\n"
     ]
    }
   ],
   "source": [
    "# Test 2\n",
    "estado = [19, 3, 1]\n",
    "model = model_ppo\n",
    "\n",
    "print(f\"Estado: {estado}\")\n",
    "print(f\"Acción: {accion_agente(model, estado)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este segundo caso, el agente tiene 2 cartas que suman 19, siendo una de ellas un as, mientras que el *dealer*, con una sola carta, tiene 3 puntos. Lo lógico en este caso es quedarse, ya que puede ser que se robe una carta con valor mayor que 2, pasándose así del límite de 21 y perdiendo el juego. Es justamente esta acción que el agente decide realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta acá`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_actions: Box(-1.0, 1.0, (2,), float32)\n",
      "dim_states: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "dim_actions = env.action_space # dimensión de las acciones\n",
    "dim_states = env.observation_space # dimensión de los estados (observaciones)\n",
    "\n",
    "print('dim_actions:', dim_actions)\n",
    "print('dim_states:', dim_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -262.19\n",
      "Desviación estándar de las recompensas: 147.82\n"
     ]
    }
   ],
   "source": [
    "# Reiniciamos el ambiente\n",
    "env.reset()\n",
    "\n",
    "# Parámetros de la simulación\n",
    "episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# Simulación\n",
    "for _ in range(episodes):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()                      # Se toma una acción aleatoria\n",
    "        obs, reward, done, truncated, info = env.step(action)   # Se realiza la acción\n",
    "        total_reward += reward                                  # Y se acumula la recompensa del episodio\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Cálculo del promedio y la desviación estándar\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "# Reporte\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a usar el modelo `PPO` dado que también sirve para problemas de este tipo `box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f168e1109642bcb8b411de2a6f3d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2396ef33190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciamos el modelo\n",
    "model_ppo = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    seed=30\n",
    ")\n",
    "\n",
    "# Reiniciamos el ambiente\n",
    "env.reset()\n",
    "\n",
    "# Parámetros de la simulación\n",
    "episodes = 10\n",
    "model = model_ppo\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.learn(total_timesteps=10000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio tras entrenamiento: -106.75\n",
      "Desviación estándar: 134.77\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
    "print(f\"Recompensa promedio tras entrenamiento: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aItYF6sr6F_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio tras entrenamiento: 229.30\n",
      "Desviación estándar: 25.02\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo PPO\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",        # Política basada en redes neuronales\n",
    "    env,\n",
    "    learning_rate=1e-3, # Tasa de aprendizaje\n",
    "    batch_size=64,      # Tamaño del batch para entrenamiento\n",
    "    gamma=0.99,         # Factor de descuento\n",
    "    verbose=0,          # Imprimir progreso\n",
    "    seed=30             # Fijar semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.learn(total_timesteps=50000)  # Número de pasos de entrenamiento\n",
    "\n",
    "# Evaluar el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Recompensa promedio tras entrenamiento: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: AIzaSyCIHizapmi2CJSKlJvmOv6uJbNlzRyIO1E\")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: 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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [\n",
    "    'diagnostico gestion de recursos hidricos en chile_banco mundial.pdf',\n",
    "    'recursos_hidricos_en_chile.pdf'\n",
    "] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "docs = []\n",
    "for doc_path in doc_paths:\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    doc = loader.load()\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Inicializamos splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Dividimos los documentos en chunks\n",
    "chunks = []\n",
    "for doc in docs:\n",
    "    split = text_splitter.split_documents(doc)\n",
    "    chunks.extend(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # inicializamos los embeddings\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding) # vectorizacion y almacenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023ABB24BB20>, search_kwargs={'k': 10})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", # método de búsqueda\n",
    "                                     search_kwargs={\"k\": 10}, # n° documentos a recuperar\n",
    "                                     )\n",
    "\n",
    "retriever_chain = retriever | format_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# noten como ahora existe el parámetro de context!\n",
    "rag_template = '''\n",
    "Eres un asistente experto en recursos hídricos y en el diagnóstico de su gestión en Chile.\n",
    "Tu único rol es contestar preguntas del usuario a partir de información relevante que te sea proporcionada.\n",
    "Responde siempre de la forma más completa posible y usando toda la información entregada.\n",
    "Responde sólo lo que te pregunten a partir de la información relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Información relevante: {context}\n",
    "Pregunta: {question}\n",
    "Respuesta útil:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # modelo de lenguaje\n",
    "    temperature=0, # probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # sin tope de tokens\n",
    "    timeout=None, # sin timeout\n",
    "    max_retries=2, # número máximo de intentos\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasará directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos sólo la respuesta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    (\"¿Qué desafíos existen en la gestión de recursos hídricos en Chile?\", \n",
    "     \"Algunos desafíos son la sobreexplotación, la falta de acceso equitativo y la gestión ineficiente.\"),\n",
    "    (\"¿Qué entidad regula el uso del agua en Chile?\", \n",
    "     \"La Dirección General de Aguas (DGA) regula el uso del agua en Chile.\"),\n",
    "    (\"¿Cuál es el principal sector que utiliza agua en Chile?\", \n",
    "     \"El sector agrícola es el principal consumidor de agua en Chile.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué desafíos existen en la gestión de recursos hídricos en Chile?\n",
      "Respuesta generada: Basado en la información proporcionada, los desafíos en la gestión de recursos hídricos en Chile se dividen en dos categorías principales:  aspectos legales e instrumentos de gestión, y aspectos institucionales.\n",
      "\n",
      "**Desafíos vinculados a los aspectos legales e instrumentos de gestión:** El informe identifica ocho desafíos principales en esta área, aunque no los detalla.  Se menciona específicamente la necesidad de proteger los derechos de agua de los grupos vulnerables.\n",
      "\n",
      "**Desafíos vinculados a los aspectos institucionales:**  El informe indica que los principales problemas institucionales se relacionan con la baja jerarquía y capacidad de las instituciones para planificar y tener injerencia en la gestión del agua, asegurando que todos los usos sean respetados.  Se destaca la ausencia de un elenco de usos prioritarios en caso de escasez.  Adicionalmente, se mencionan brechas en la calidad del agua relacionadas con la falta de una política para la gestión integrada de recursos hídricos a nivel de cuenca, la ausencia de instancias de participación de expertos en la toma de decisiones, y la falta de información actualizada sobre aguas subterráneas.\n",
      "\n",
      "Finalmente, el informe menciona que existen 14 desafíos principales en total, algunos específicos del sistema chileno y otros similares a los de países con un nivel socioeconómico y recursos hídricos similares.  Sin embargo, la información proporcionada no detalla estos 14 desafíos.\n",
      "\n",
      "Respuesta esperada: Algunos desafíos son la sobreexplotación, la falta de acceso equitativo y la gestión ineficiente.\n",
      "--------------------------------------------------------------------------------\n",
      "Pregunta: ¿Qué entidad regula el uso del agua en Chile?\n",
      "Respuesta generada: En Chile, el uso del agua está regulado principalmente por la Constitución Política de la República y el Código de Aguas de 1981.  La Dirección General de Aguas (DGA) tiene la responsabilidad principal de la supervisión de la gestión de recursos hídricos del país.  Adicionalmente, la Comisión Nacional de Riego (CNR) elabora políticas y programas para el subsector del riego, el mayor uso del agua en Chile.  Finalmente, el Ministerio del Medio Ambiente (MMA) es responsable de la calidad del agua.\n",
      "\n",
      "Respuesta esperada: La Dirección General de Aguas (DGA) regula el uso del agua en Chile.\n",
      "--------------------------------------------------------------------------------\n",
      "Pregunta: ¿Cuál es el principal sector que utiliza agua en Chile?\n",
      "Respuesta generada: El principal sector usuario de agua en Chile es el sector agrícola, con extracciones de alrededor del 73%.\n",
      "\n",
      "Respuesta esperada: El sector agrícola es el principal consumidor de agua en Chile.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for question, expected_answer in questions_answers:\n",
    "    # Invocar el RAG Chain con la pregunta\n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    # Mostrar la pregunta, la respuesta generada y la respuesta esperada\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta generada: {response}\")\n",
    "    print(f\"Respuesta esperada: {expected_answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, el `RaG` desarrollado entregas las respuestas que se esperaban y las desarrolló aún más ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 5) # inicializamos tool\n",
    "tavi_tool = [search] # guardamos las tools en una lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=100)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tavi_template = \"\"\"\n",
    "Responde las siguientes preguntas lo mejor que puedas. Tienes acceso a las siguientes herramientas:\n",
    "\n",
    "{tavi_tool}\n",
    "\n",
    "Usa el siguiente formato:\n",
    "\n",
    "Pregunta: la pregunta de entrada que debes responder\n",
    "Pensamiento: siempre debes pensar en lo que debes hacer\n",
    "Acción: la acción a realizar debe ser una de [{tavi_tool}]\n",
    "Entrada de acción: la entrada a la acción\n",
    "Observación: el resultado de la acción\n",
    "... (este Pensamiento/Acción/Entrada de acción/Observación puede repetirse N veces)\n",
    "Pensamiento: ahora sé la respuesta final\n",
    "Respuesta final: la respuesta final a la pregunta de entrada original\n",
    "\n",
    "¡Comienza!\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_wiki_template = \"\"\"\n",
    "Responde las siguientes preguntas lo mejor que puedas. Tienes acceso a las siguientes herramientas:\n",
    "\n",
    "{wiki_tool}\n",
    "\n",
    "Usa el siguiente formato:\n",
    "\n",
    "Pregunta: la pregunta de entrada que debes responder\n",
    "Pensamiento: siempre debes pensar en lo que debes hacer\n",
    "Acción: la acción a realizar debe ser una de [{wiki_tool}]\n",
    "Entrada de acción: la entrada a la acción\n",
    "Observación: el resultado de la acción\n",
    "... (este Pensamiento/Acción/Entrada de acción/Observación puede repetirse N veces)\n",
    "Pensamiento: ahora sé la respuesta final\n",
    "Respuesta final: la respuesta final a la pregunta de entrada original\n",
    "\n",
    "¡Comienza!\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_react_agent, AgentExecutor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Agente para Tavily\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tavily_agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_react_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtavi_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tavi_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m tavily_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39mtavily_agent, tools\u001b[38;5;241m=\u001b[39mtavi_tool, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Agente para Wikipedia\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\agents\\react\\agent.py:120\u001b[0m, in \u001b[0;36mcreate_react_agent\u001b[1;34m(llm, tools, prompt, output_parser, tools_renderer, stop_sequence)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_react_agent\u001b[39m(\n\u001b[0;32m     17\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m     18\u001b[0m     tools: Sequence[BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     stop_sequence: Union[\u001b[38;5;28mbool\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an agent that uses ReAct prompting.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Based on paper \"ReAct: Synergizing Reasoning and Acting in Language Models\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m            prompt = PromptTemplate.from_template(template)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     missing_vars \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m}\u001b[38;5;241m.\u001b[39mdifference(\n\u001b[1;32m--> 120\u001b[0m         \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mpartial_variables)\n\u001b[0;32m    121\u001b[0m     )\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt missing required variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'input_variables'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "# Agente para Tavily\n",
    "tavily_agent = create_react_agent(llm, tavi_tool, prompt_tavi_template)\n",
    "tavily_executor = AgentExecutor(agent=tavily_agent, tools=tavi_tool, verbose=True)\n",
    "\n",
    "# Agente para Wikipedia\n",
    "wikipedia_agent = create_react_agent(llm, tavi_tool, prompt_wiki_template)\n",
    "wikipedia_executor = AgentExecutor(agent=wikipedia_agent, tools=tavi_tool, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": [
    "response = tavily_executor.invoke({\"input\": \"qué equipo ganó el mundial de LoL 2024?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta acá`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
