{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Aut칩nomos 游뱄**\n",
    "\n",
    "MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti치n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol치s Ojeda, Melanie Pe침a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Elizabeth Ram칤rez Z.\n",
    "- Nombre de alumno 2: Lucas Orellana J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/ElLuquitas/MDS7202-labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci칩n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas 칰tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci칩n van a usar m칠todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci칩n es que puedan implementar m칠todos de RL y as칤 generar una estrategia para jugar el cl치sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c칩digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci칩n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci칩n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_actions: 2\n",
      "dim_states: MultiDiscrete([32 11  2])\n"
     ]
    }
   ],
   "source": [
    "dim_actions = env.action_space.n # dimensi칩n de las acciones\n",
    "dim_states = env.observation_space # dimensi칩n de los estados (observaciones)\n",
    "\n",
    "print('dim_actions:', dim_actions)\n",
    "print('dim_states:', dim_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente **Blackjack** consiste en una recreaci칩n del juego de cartas del mismo nombre, en donde el agente busca vencer al *dealer* obteniendo un conjunto de cartas que sumen $21$ (sin sobrepasar este n칰mero) a la vez que esta suma es mayor a la obtenida por el *dealer*. Su formulaci칩n en MDP consta de 3 puntos clave:\n",
    "\n",
    "- Estados: cada uno de los estados est치 descrito como una tupla de tres elementos `(sum_hand, dealer_card, usable_ace)`, en donde:\n",
    "    - `sum_hand` representa la suma de las cartas actuales del agente, con un total de $32$ posibilidades.\n",
    "    - `dealer_card` representa la carta boca arriba con la que empieza el *dealer*, con un total de $11$ posibilidades.\n",
    "    - `usable_ace` representa si el agente siene alg칰n as, el cual puede contar como $11$ en caso de que la suma (considerando el as como $11$) no exceda el valor $21$, con un total de $2$ posibilidades.\n",
    "\n",
    "    Esto nos da un total de $32 \\cdot 11 \\cdot 2 = 704$ estados.\n",
    "\n",
    "- Acciones: Dada la naturaleza del juego, el agente puede ejecutar dos acciones:\n",
    "    - `0` significa que el agente se queda con las cartas que tiene en mano (stick)\n",
    "    - `1` significa que el agente toma auna carta adicional (hit)\n",
    "\n",
    "- Recompensas: Las recompensas tienen que ver directamnte con el resultado del juego:\n",
    "    - `+1` si el agente le gana al *dealer* sin un blackjack natural.\n",
    "    - `-1` si el agente pierde contra el *dealer*.\n",
    "    - `0` si ambos empatan.\n",
    "    - `+1.5` si el agente le gana al *dealer* con un blackjack natural (lo obtiene en la mano inicial)\n",
    "\n",
    "El episodio (o la experiencia) termina en dos casos:\n",
    "1. El agente decide robar una carta m치s y la suma de su mano excede el valor $21$.\n",
    "2. El agente se queda con su mano actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 5000 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica? 쮺칩mo podr칤a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -0.41\n",
      "Desviaci칩n est치ndar de las recompensas: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Reiniciamos el ambiente (por si acaso)\n",
    "env.reset()\n",
    "\n",
    "# Par치metros de la simulaci칩n\n",
    "episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "# Simulaci칩n\n",
    "for _ in range(episodes):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()                      # Se toma una acci칩n aleatoria\n",
    "        obs, reward, done, truncated, info = env.step(action)   # Se realiza la acci칩n\n",
    "        total_reward += reward                                  # Y se acumula la recompensa del episodio\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# C치lculo del promedio y la desviaci칩n est치ndar\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "# Reporte\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se expone en los resultados, el performance es bastante malo, ya que el agente est치 perdiendo, en promedio, 0.42 unidades de rrecompensa por cada episodio, lo cual indica que toma decisiones sub칩ptimas. Adem치s, la desviaci칩n est치ndar refleja la alta variabilidad en este juego: como las acciones del agente son aleatorias, se ingresa mucha incertidumbre, teniendo por resultado una alta variabilidad en los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que este es un entorno multidiscreto, lo ideal ser칤a usar un modelo del tipo `PPO`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23395da0dd3a42abadaa9abedfa5d3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2395027f730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Iniciamos el modelo\n",
    "model_ppo = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    seed=30\n",
    ")\n",
    "\n",
    "# Reiniciamos el ambiente\n",
    "env.reset()\n",
    "\n",
    "# Par치metros de la simulaci칩n\n",
    "episodes = 5000\n",
    "model = model_ppo\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.learn(total_timesteps=50000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio tras entrenamiento: -0.03\n",
      "Desviaci칩n est치ndar: 0.95\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Evaluamos el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5000)\n",
    "print(f\"Recompensa promedio tras entrenamiento: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci칩n est치ndar: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos nuevos resultados reflejan una mejora significativa en las acciones del agente. Aunque la recompensa promedio haya aumentado, sigue siendo negativa, pero por lo menos el agente ya no pierde tanto. La desviaci칩n est치ndar sigue siendo alta, pero es t칤pico para estos juegos de azar.\n",
    "\n",
    "Es posible ense침arle al agente c칩mo proceder en algunas situaciones para disminuir esta varianza tan alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci칩n que reciba un estado y retorne la accion del agente. Luego, use esta funci칩n para entregar la acci칩n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "쯉on coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: 쮸 que clase de python pertenecen los estados? Pruebe a usar el m칠todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "def accion_agente(model, estado):\n",
    "    '''\n",
    "    Funci칩n que devuelve la acci칩n que el agente tomar치 en un estado dado.\n",
    "\n",
    "    Args:\n",
    "        - model: Modelo entrenado.\n",
    "        - estado: Estado en el que se encuentra el agente.\n",
    "\n",
    "    Returns:\n",
    "        - acci칩n: Acci칩n que tomar치 el agente.\n",
    "    '''\n",
    "    # El estado se debe pasar como un array de una dimensi칩n\n",
    "    estado = np.array(estado).flatten()\n",
    "\n",
    "    accion, _ = model.predict(estado, deterministic=True)\n",
    "    return accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado: [6, 7, 0]\n",
      "Acci칩n: 1\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "estado = [6, 7, 0]\n",
    "model = model_ppo\n",
    "\n",
    "print(f\"Estado: {estado}\")\n",
    "print(f\"Acci칩n: {accion_agente(model, estado)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este primer caso, el agente tiene 2 cartas que suman 3 puntos, mientras que el *dealer*, con una sola carta, ya tiene 7 puntos. Lo l칩gico en este caso es robar una carta ya que, independiente del valor que se obtenga, no sobrepasar치 lo 21. Es justamente esta acci칩n que el agente decide realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado: [19, 3, 1]\n",
      "Acci칩n: 0\n"
     ]
    }
   ],
   "source": [
    "# Test 2\n",
    "estado = [19, 3, 1]\n",
    "model = model_ppo\n",
    "\n",
    "print(f\"Estado: {estado}\")\n",
    "print(f\"Acci칩n: {accion_agente(model, estado)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este segundo caso, el agente tiene 2 cartas que suman 19, siendo una de ellas un as, mientras que el *dealer*, con una sola carta, tiene 3 puntos. Lo l칩gico en este caso es quedarse, ya que puede ser que se robe una carta con valor mayor que 2, pas치ndose as칤 del l칤mite de 21 y perdiendo el juego. Es justamente esta acci칩n que el agente decide realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci칩n 2.1, en esta secci칩n usted se encargar치 de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el par치metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par치metro `continuous = True`. 쯈ue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem치s, se le facilita la funci칩n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci칩n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci칩n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci칩n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. 쮺omo se distinguen las acciones de este ambiente en comparaci칩n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific칩 el par치metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac치`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_actions: Box(-1.0, 1.0, (2,), float32)\n",
      "dim_states: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "dim_actions = env.action_space # dimensi칩n de las acciones\n",
    "dim_states = env.observation_space # dimensi칩n de los estados (observaciones)\n",
    "\n",
    "print('dim_actions:', dim_actions)\n",
    "print('dim_states:', dim_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 10 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -262.19\n",
      "Desviaci칩n est치ndar de las recompensas: 147.82\n"
     ]
    }
   ],
   "source": [
    "# Reiniciamos el ambiente\n",
    "env.reset()\n",
    "\n",
    "# Par치metros de la simulaci칩n\n",
    "episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# Simulaci칩n\n",
    "for _ in range(episodes):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()                      # Se toma una acci칩n aleatoria\n",
    "        obs, reward, done, truncated, info = env.step(action)   # Se realiza la acci칩n\n",
    "        total_reward += reward                                  # Y se acumula la recompensa del episodio\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# C치lculo del promedio y la desviaci칩n est치ndar\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "# Reporte\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a usar el modelo `PPO` dado que tambi칠n sirve para problemas de este tipo `box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f168e1109642bcb8b411de2a6f3d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2396ef33190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciamos el modelo\n",
    "model_ppo = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    seed=30\n",
    ")\n",
    "\n",
    "# Reiniciamos el ambiente\n",
    "env.reset()\n",
    "\n",
    "# Par치metros de la simulaci칩n\n",
    "episodes = 10\n",
    "model = model_ppo\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.learn(total_timesteps=10000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio tras entrenamiento: -106.75\n",
      "Desviaci칩n est치ndar: 134.77\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
    "print(f\"Recompensa promedio tras entrenamiento: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci칩n est치ndar: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par치metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci칩n `export_gif` para estudiar el comportamiento de su agente en la resoluci칩n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a칰n si adem치s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aItYF6sr6F_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio tras entrenamiento: 229.30\n",
      "Desviaci칩n est치ndar: 25.02\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo PPO\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",        # Pol칤tica basada en redes neuronales\n",
    "    env,\n",
    "    learning_rate=1e-3, # Tasa de aprendizaje\n",
    "    batch_size=64,      # Tama침o del batch para entrenamiento\n",
    "    gamma=0.99,         # Factor de descuento\n",
    "    verbose=0,          # Imprimir progreso\n",
    "    seed=30             # Fijar semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.learn(total_timesteps=50000)  # N칰mero de pasos de entrenamiento\n",
    "\n",
    "# Evaluar el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Recompensa promedio tras entrenamiento: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci칩n est치ndar: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci칩n se enfocar치n en habilitar un Chatbot que nos permita responder preguntas 칰tiles a trav칠s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci칩n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: AIzaSyCIHizapmi2CJSKlJvmOv6uJbNlzRyIO1E\")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: 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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci칩n es que habiliten un chatbot que pueda responder preguntas usando informaci칩n contenida en documentos PDF a trav칠s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m칤nimo.\n",
    "  - 50 p치ginas de contenido como m칤nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad칠micos, laborales o de ocio. Aprovechen este ejercicio para construir algo 칰til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [\n",
    "    'diagnostico gestion de recursos hidricos en chile_banco mundial.pdf',\n",
    "    'recursos_hidricos_en_chile.pdf'\n",
    "] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m칤nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P치ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "docs = []\n",
    "for doc_path in doc_paths:\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    doc = loader.load()\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Inicializamos splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Dividimos los documentos en chunks\n",
    "chunks = []\n",
    "for doc in docs:\n",
    "    split = text_splitter.split_documents(doc)\n",
    "    chunks.extend(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # inicializamos los embeddings\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding) # vectorizacion y almacenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci칩n RAG a trav칠s de una *chain* y gu치rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023ABB24BB20>, search_kwargs={'k': 10})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", # m칠todo de b칰squeda\n",
    "                                     search_kwargs={\"k\": 10}, # n춿 documentos a recuperar\n",
    "                                     )\n",
    "\n",
    "retriever_chain = retriever | format_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# noten como ahora existe el par치metro de context!\n",
    "rag_template = '''\n",
    "Eres un asistente experto en recursos h칤dricos y en el diagn칩stico de su gesti칩n en Chile.\n",
    "Tu 칰nico rol es contestar preguntas del usuario a partir de informaci칩n relevante que te sea proporcionada.\n",
    "Responde siempre de la forma m치s completa posible y usando toda la informaci칩n entregada.\n",
    "Responde s칩lo lo que te pregunten a partir de la informaci칩n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Informaci칩n relevante: {context}\n",
    "Pregunta: {question}\n",
    "Respuesta 칰til:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # modelo de lenguaje\n",
    "    temperature=0, # probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # sin tope de tokens\n",
    "    timeout=None, # sin timeout\n",
    "    max_retries=2, # n칰mero m치ximo de intentos\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasar치 directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos s칩lo la respuesta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci칩n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci칩n para cada una. 쯉u soluci칩n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: 쯈ui칠n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    (\"쯈u칠 desaf칤os existen en la gesti칩n de recursos h칤dricos en Chile?\", \n",
    "     \"Algunos desaf칤os son la sobreexplotaci칩n, la falta de acceso equitativo y la gesti칩n ineficiente.\"),\n",
    "    (\"쯈u칠 entidad regula el uso del agua en Chile?\", \n",
    "     \"La Direcci칩n General de Aguas (DGA) regula el uso del agua en Chile.\"),\n",
    "    (\"쮺u치l es el principal sector que utiliza agua en Chile?\", \n",
    "     \"El sector agr칤cola es el principal consumidor de agua en Chile.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: 쯈u칠 desaf칤os existen en la gesti칩n de recursos h칤dricos en Chile?\n",
      "Respuesta generada: Basado en la informaci칩n proporcionada, los desaf칤os en la gesti칩n de recursos h칤dricos en Chile se dividen en dos categor칤as principales:  aspectos legales e instrumentos de gesti칩n, y aspectos institucionales.\n",
      "\n",
      "**Desaf칤os vinculados a los aspectos legales e instrumentos de gesti칩n:** El informe identifica ocho desaf칤os principales en esta 치rea, aunque no los detalla.  Se menciona espec칤ficamente la necesidad de proteger los derechos de agua de los grupos vulnerables.\n",
      "\n",
      "**Desaf칤os vinculados a los aspectos institucionales:**  El informe indica que los principales problemas institucionales se relacionan con la baja jerarqu칤a y capacidad de las instituciones para planificar y tener injerencia en la gesti칩n del agua, asegurando que todos los usos sean respetados.  Se destaca la ausencia de un elenco de usos prioritarios en caso de escasez.  Adicionalmente, se mencionan brechas en la calidad del agua relacionadas con la falta de una pol칤tica para la gesti칩n integrada de recursos h칤dricos a nivel de cuenca, la ausencia de instancias de participaci칩n de expertos en la toma de decisiones, y la falta de informaci칩n actualizada sobre aguas subterr치neas.\n",
      "\n",
      "Finalmente, el informe menciona que existen 14 desaf칤os principales en total, algunos espec칤ficos del sistema chileno y otros similares a los de pa칤ses con un nivel socioecon칩mico y recursos h칤dricos similares.  Sin embargo, la informaci칩n proporcionada no detalla estos 14 desaf칤os.\n",
      "\n",
      "Respuesta esperada: Algunos desaf칤os son la sobreexplotaci칩n, la falta de acceso equitativo y la gesti칩n ineficiente.\n",
      "--------------------------------------------------------------------------------\n",
      "Pregunta: 쯈u칠 entidad regula el uso del agua en Chile?\n",
      "Respuesta generada: En Chile, el uso del agua est치 regulado principalmente por la Constituci칩n Pol칤tica de la Rep칰blica y el C칩digo de Aguas de 1981.  La Direcci칩n General de Aguas (DGA) tiene la responsabilidad principal de la supervisi칩n de la gesti칩n de recursos h칤dricos del pa칤s.  Adicionalmente, la Comisi칩n Nacional de Riego (CNR) elabora pol칤ticas y programas para el subsector del riego, el mayor uso del agua en Chile.  Finalmente, el Ministerio del Medio Ambiente (MMA) es responsable de la calidad del agua.\n",
      "\n",
      "Respuesta esperada: La Direcci칩n General de Aguas (DGA) regula el uso del agua en Chile.\n",
      "--------------------------------------------------------------------------------\n",
      "Pregunta: 쮺u치l es el principal sector que utiliza agua en Chile?\n",
      "Respuesta generada: El principal sector usuario de agua en Chile es el sector agr칤cola, con extracciones de alrededor del 73%.\n",
      "\n",
      "Respuesta esperada: El sector agr칤cola es el principal consumidor de agua en Chile.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for question, expected_answer in questions_answers:\n",
    "    # Invocar el RAG Chain con la pregunta\n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    # Mostrar la pregunta, la respuesta generada y la respuesta esperada\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta generada: {response}\")\n",
    "    print(f\"Respuesta esperada: {expected_answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, el `RaG` desarrollado entregas las respuestas que se esperaban y las desarroll칩 a칰n m치s ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar치metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an치lisis del punto 2.1.4 analizando c칩mo cambian las respuestas entregadas cambiando los siguientes hiperpar치metros:\n",
    "- `Tama침o del chunk`. (*쮺칩mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*쯈u칠 pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b칰squeda`. (*쮺칩mo afecta el tipo de b칰squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci칩n anterior, en esta secci칩n se busca habilitar **Agentes** para obtener informaci칩n a trav칠s de tools y as칤 responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b칰squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 5) # inicializamos tool\n",
    "tavi_tool = [search] # guardamos las tools en una lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=100)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg칰rese que su agente responda en espa침ol. Por 칰ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tavi_template = \"\"\"\n",
    "Responde las siguientes preguntas lo mejor que puedas. Tienes acceso a las siguientes herramientas:\n",
    "\n",
    "{tavi_tool}\n",
    "\n",
    "Usa el siguiente formato:\n",
    "\n",
    "Pregunta: la pregunta de entrada que debes responder\n",
    "Pensamiento: siempre debes pensar en lo que debes hacer\n",
    "Acci칩n: la acci칩n a realizar debe ser una de [{tavi_tool}]\n",
    "Entrada de acci칩n: la entrada a la acci칩n\n",
    "Observaci칩n: el resultado de la acci칩n\n",
    "... (este Pensamiento/Acci칩n/Entrada de acci칩n/Observaci칩n puede repetirse N veces)\n",
    "Pensamiento: ahora s칠 la respuesta final\n",
    "Respuesta final: la respuesta final a la pregunta de entrada original\n",
    "\n",
    "춰Comienza!\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_wiki_template = \"\"\"\n",
    "Responde las siguientes preguntas lo mejor que puedas. Tienes acceso a las siguientes herramientas:\n",
    "\n",
    "{wiki_tool}\n",
    "\n",
    "Usa el siguiente formato:\n",
    "\n",
    "Pregunta: la pregunta de entrada que debes responder\n",
    "Pensamiento: siempre debes pensar en lo que debes hacer\n",
    "Acci칩n: la acci칩n a realizar debe ser una de [{wiki_tool}]\n",
    "Entrada de acci칩n: la entrada a la acci칩n\n",
    "Observaci칩n: el resultado de la acci칩n\n",
    "... (este Pensamiento/Acci칩n/Entrada de acci칩n/Observaci칩n puede repetirse N veces)\n",
    "Pensamiento: ahora s칠 la respuesta final\n",
    "Respuesta final: la respuesta final a la pregunta de entrada original\n",
    "\n",
    "춰Comienza!\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_react_agent, AgentExecutor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Agente para Tavily\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tavily_agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_react_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtavi_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tavi_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m tavily_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39mtavily_agent, tools\u001b[38;5;241m=\u001b[39mtavi_tool, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Agente para Wikipedia\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\agents\\react\\agent.py:120\u001b[0m, in \u001b[0;36mcreate_react_agent\u001b[1;34m(llm, tools, prompt, output_parser, tools_renderer, stop_sequence)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_react_agent\u001b[39m(\n\u001b[0;32m     17\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m     18\u001b[0m     tools: Sequence[BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     stop_sequence: Union[\u001b[38;5;28mbool\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an agent that uses ReAct prompting.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Based on paper \"ReAct: Synergizing Reasoning and Acting in Language Models\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m            prompt = PromptTemplate.from_template(template)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     missing_vars \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m}\u001b[38;5;241m.\u001b[39mdifference(\n\u001b[1;32m--> 120\u001b[0m         \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mpartial_variables)\n\u001b[0;32m    121\u001b[0m     )\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt missing required variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'input_variables'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "# Agente para Tavily\n",
    "tavily_agent = create_react_agent(llm, tavi_tool, prompt_tavi_template)\n",
    "tavily_executor = AgentExecutor(agent=tavily_agent, tools=tavi_tool, verbose=True)\n",
    "\n",
    "# Agente para Wikipedia\n",
    "wikipedia_agent = create_react_agent(llm, tavi_tool, prompt_wiki_template)\n",
    "wikipedia_executor = AgentExecutor(agent=wikipedia_agent, tools=tavi_tool, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci칩n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg칰rese que el agente est칠 ocupando correctamente las tools disponibles. 쮼n qu칠 casos el agente deber칤a ocupar la tool de Tavily? 쮼n qu칠 casos deber칤a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": [
    "response = tavily_executor.invoke({\"input\": \"qu칠 equipo gan칩 el mundial de LoL 2024?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci칩n es encapsular las funcionalidades creadas en una soluci칩n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci칩n RAG de la secci칩n 2.1 y el agente de la secci칩n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci칩n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. 쮺칩mo var칤an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An치lisis (0.25 puntos)**\n",
    "\n",
    "쯈u칠 diferencias tiene este enfoque con la soluci칩n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta ac치`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti치n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti치n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti치n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci칩n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v치lido <u>s칩lo para la secci칩n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav칠s de `gradio`, una librer칤a especializada en el levantamiento r치pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer칤a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s칩lo deben ejecutar el siguiente c칩digo e interactuar con la interfaz a trav칠s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci칩n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy 칰til :)\", # tambi칠n la descripci칩n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
